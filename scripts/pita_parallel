#!/usr/bin/env python
from pita.model import get_chrom_models
from pita.log import AnnotationLog
import os
import sys
import logging
import yaml
import argparse
import pysam
import subprocess
import pp
from tempfile import NamedTemporaryFile
import multiprocessing as mp
from functools import partial
import signal

DEFAULT_CONFIG = "pita.yaml"
VALID_TYPES = ["bed", "gff", "gff3", "gtf"]

# Setup logging
formatter = logging.Formatter(fmt='%(asctime)s - %(levelname)s - %(module)s - %(message)s')
handler = logging.StreamHandler()
handler.setFormatter(formatter)
logger = logging.getLogger("pita")
logger.setLevel(logging.DEBUG)
logger.addHandler(handler)

p = argparse.ArgumentParser()
p.add_argument("-c",
               dest= "configfile",
               default = DEFAULT_CONFIG,
               help="Configuration file (default: {0})".format(DEFAULT_CONFIG)
              )

args = p.parse_args()
configfile = args.configfile

# Parse YAML config file
f = open(configfile, "r")
config = yaml.load(f)

# Data directory
base = "."
if config.has_key("data_path"):
    base = config["data_path"]

if not config.has_key("annotation") or len(config["annotation"]) == 0:
    print "No annotation files specified."
    sys.exit(1)

anno_files = []
chroms = {}
for d in config["annotation"]:
    logger.debug("annotation: {0}".format(d))
    fname = os.path.join(base, d["path"])
    t = d["type"].lower()
    if not t in VALID_TYPES:
        print "Invalid type: {0}".format(t)
        sys.exit(1)
    if not os.path.exists(fname):
        print "File does not exist: {0}".format(fname)
        sys.exit(1)
    else:
        logger.info("Preparing {0} for tabix".format(fname))
        tmp = NamedTemporaryFile()
        preset = "gff"
        if t == "bed":
            cmd = "sort -k1,1 -k2g,2 {0} | grep -v track | grep -v \"^#\" > {1}"
            preset = "bed"
        elif t in ["gff", "gff3", "gtf3"]:
            cmd = "sort -k1,1 -k4g,4 {0} | grep -v \"^#\" > {1}"
        
        # Sort the input file
        logger.debug(cmd.format(fname, tmp.name))
        subprocess.call(cmd.format(fname, tmp.name), shell=True)
        # Compress using bgzip
        logger.debug("compressing {0}".format(tmp.name))
        tabix_file = tmp.name + ".gz"
        pysam.tabix_compress(tmp.name, tabix_file)
        # Index (using tabix command line, as pysam.index results in a Segmentation fault
        logger.debug("indexing {0}".format(tabix_file))
        subprocess.call("tabix {0} -p {1}".format(tabix_file, preset), shell=True)
        # Add file info
        anno_files.append([d["name"], tabix_file, t])
        # Save chromosome names
        for chrom in pysam.Tabixfile(tabix_file).contigs:
            chroms[chrom] = 1

# data  config
data = []
if config.has_key("data") and config["data"]:
    for d in config["data"]:
        logger.debug("data: {0}".format(d))
        d.setdefault("up", 0)
        d.setdefault("down", 0)
        row = [d["name"], os.path.join(base, d["path"]), d["feature"], (d["up"], d["down"])]
        data.append(row)

weight = {}
if config.has_key("scoring"):
    weight = config["scoring"]

line_format = "{0}\t{1}\t{2}\t{3}\t{4}\t{5}\t{6}\t{7}\t{8}\t{9}\t{10}\t{11}"
print 'track name="pita_test"'
chroms = chroms.keys()
#chroms = ["JGIv7b.000000004", "JGIv7b.000087017", "JGIv7b.000012518"]
#chroms = ["JGIv7b.000000004"]
#chroms = ["JGIv7b.000287959"]
#chroms = ["JGIv7b.000007440"]

def init_worker():
    signal.signal(signal.SIGINT, signal.SIG_IGN)

def listener(q, names, append=False):
    '''listens for messages on the q, writes to file. '''

    alog = AnnotationLog(append)
    for name in names:
        alog.add(name)
    
    while 1:
        m = q.get()
        if m == 'kill':
            break
       
        genename, exons,  ev, best_ev, other_ev = m
        chrom = exons[0].chr
        chromStart = exons[0].start
        chromEnd = exons[-1].end
        sizes = ",".join([str(exon.end - exon.start) for exon in exons]) + ","
        starts = ",".join([str(exon.start - chromStart) for exon in exons]) + ","
        print line_format.format(
                                chrom, 
                                chromStart, 
                                chromEnd, 
                                genename, 
                                600, 
                                exons[0].strand, 
                                chromStart, 
                                chromEnd, 
                                "0,0,0", 
                                len(exons), 
                                sizes, 
                                starts
                                )
        alog.log_to_file(genename, exons,  ev, best_ev, other_ev ) 

def annotate_chrom(chrom, q, anno_files, data, weight):
    logger.info("Chromosome {0} started".format(chrom))
    for genename,best_exons,other_exons in get_chrom_models(chrom, anno_files, data, weight):
        ### Ugly logging stuff
        best_ev = {}
        for e in best_exons:
            for ev in set([x.split(":")[0] for x in e.evidence]):
                best_ev[ev] = best_ev.setdefault(ev, 0) + 1
        other_ev = {}
        
        # Fast way to collapse
        for e in other_exons:
            for ev in set([x.split(":")[0] for x in e.evidence]):
                other_ev[ev] = other_ev.setdefault(ev, 0) + 1
        ev = []
        for e in best_exons + other_exons:
            for evidence in e.evidence:
                ev.append(evidence.split(":"))
        
        ### End ugly logging stuff

        q.put([genename, best_exons, ev, best_ev, other_ev])

    logger.info("Chromosome {0} finished".format(chrom))

manager = mp.Manager()
q = manager.Queue()    

pool = mp.Pool(7, init_worker) 
try:    
    partialAnnotate = partial(annotate_chrom, q=q, anno_files=anno_files, data=data, weight=weight)
    #put listener to work first
    watcher = pool.apply_async(listener, (q,[x[0] for x in anno_files], False))
    pool.map(partialAnnotate, chroms) 
    q.put('kill')
    pool.close()
    pool.join()
except KeyboardInterrupt:
    print "Caught KeyboardInterrupt, terminating workers"
    pool.terminate()
    pool.join()
